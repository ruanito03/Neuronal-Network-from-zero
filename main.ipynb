{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de librerías:\n",
    "\n",
    "En primer lugar, importo tensoflow para acceder a la base de datos de MNIST y poder trabajar con ella. Además, importo numpy para trabajar con arrays y matplotlib para visualizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 15:26:09.960451: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 15:26:10.817234: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-10 15:26:12.799984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 15:26:14.790582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien importo una librería con las funciones de activación y sus derivadas, necesarias tanto para la Forward como para la Backward Propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'activ_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mactiv_func\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m softmax, sigmoid, sigmoid_p\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'activ_func'"
     ]
    }
   ],
   "source": [
    "from activ_func import softmax, sigmoid, sigmoid_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación y normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train/np.max(x_train) \n",
    "x_test = x_test/np.max(x_test)\n",
    "x_train_reshaped = x_train.reshape(60000,784)\n",
    "x_test_reshaped = x_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de un dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1], cmap=plt.cm.binary)\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienzo la implementación de las funciones necesarias para que funcione la red neuronal. Comenzando por el Forward Propagation, donde se calcula la salida de la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino las funciones que permiten el forward propagation de la informacion a traves de la red y calcula al final la matriz derivada de la pérdida:\n",
    "\n",
    "$\\delta^{(2)}=\\frac{\\partial L}{\\partial z^{(2)}}$ \n",
    "\n",
    "En cada fila contiene el vector derivada de pérdida para cada elemento (imágen) del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(a_f,y):\n",
    "    delta_2 = a_f - y\n",
    "    return delta_2\n",
    "\n",
    "def Forward_prop(a_0, g1, W1, b1, g2, W2, b2, y):\n",
    "    z_1 = np.matmul(a_0, W1) + b1\n",
    "    a_1 = g1(z_1)\n",
    "    z_2 = np.matmul(a_1, W2) + b2\n",
    "    a_2 = g2(z_2)\n",
    "    delta_2 =final_loss(a_2,y)\n",
    "    return z_1, a_1, a_2, delta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino la función encoder, que me transforma y = 2 en un vector que lo representa codificado en la notación de probabilidades como $y = [0,0,1,0,0,0,0,0,0,0]$. Cada entrada representa la probabilidad de que el número sea un número del 1 al 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(y):\n",
    "    m = y.shape[0]\n",
    "    enc = np.zeros((m,10))\n",
    "    for j in range(m):\n",
    "        enc[j,y[j]] = 1\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, ya tendríamos el forward propagation. A continuación se definirán las funciones que permiten back-propagate los errores y así corregir los pesos de las neuronas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función gradiente permite estimar las matrices que representan la derivada de la pérdida promedio del batch (que predice simultáneamente n imágenes distintas) usadas para el SGD:\n",
    "\n",
    " $$\\overline{L}=\\frac{1}{n}\\sum_{i=1}^{n} L$$\n",
    "\n",
    "Partimos de que $L$ está directamente relacionado con las funciones $\\delta$ según \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{j'j}^{(l)}} = \\delta_{j'}^{(l)} a_j^{(l-1)}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_{j'}^{(l)}} = \\delta_{j'}^{(l)}$$\n",
    "\n",
    "El promedio de estas matrices se calculará simplemente haciendo \n",
    "\n",
    "$$\\frac{\\partial \\overline{L}}{\\partial w_{j'j}^{(l)}} = \\frac{1}{n}\\sum_{i=1}^{n}\\delta_{j'}^{(l)} a_j^{(l-1)}$$\n",
    "\n",
    "$$\\frac{\\partial \\overline{L}}{\\partial b_{j'}^{(l)}} = \\frac{1}{n}\\sum_{i=1}^{n}\\delta_{j'}^{(l)}$$\n",
    "\n",
    "\n",
    "Nota: la matriz $\\frac{\\partial L}{\\partial w_{j'j}^{(l)}}$ se genera mediante la multiplicacion del vector $a^{(1)}$ columna por el vector $\\delta^{(2)}$ fila; el orden es contrario al que aparece en la expresión teórica, pues para deducirla, las multiplicaciones se han realizado como matriz por vector columna, pero para implementarlas se ha multiplicado vector fila por matriz. Por lo tanto la matriz W está traspuesta respecto a lo que indica la teoría y necesito trasponer también sus actualizaciones (derivadas). Esto es equivalente a trasponer la matriz generada multiplicando $\\delta^{(2)}$ columna por $a^{(1)}$ fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(delta_f, a_i):\n",
    "    n = delta_f.shape[0]\n",
    "    dL_dw_f = np.zeros((a_i.shape[1], delta_f.shape[1]))\n",
    "    dL_db_f = np.zeros((1, delta_f.shape[1]))\n",
    "    for i in range(n):\n",
    "        dL_dw_f += np.matmul(np.array([a_i[i]]).transpose(), [delta_f[i]])/n #me calcula el promedio\n",
    "        dL_db_f += delta_f[i,:]/n\n",
    "    return dL_dw_f, dL_db_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También he implementado la función delta_i, que propaga hacia atrás el valor de $\\delta$ según la siguiente expresión:\n",
    "\n",
    "$\\delta^{(l-1)}=\\frac{dg(z_i)}{dz}\\cdot(W^{(l)})^T\\cdot\\delta^{(l)}$\n",
    "\n",
    "Nota: la multiplicación por la derivada de la función de activación es componente a componente y no producto escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_i(delta_f, dg_dz_i, z_i, W_f):\n",
    "    return dg_dz_i(z_i) * np.matmul(delta_f, W_f.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez conocido el gradiente puedo crear la funcion Backward_prop, que corrige los pesos de las neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_prop(delta_2, W2, b2, delta_1, W1, b1, a_1, a_0, alpha1, alpha2):  # alpha son los learning rate\n",
    "    dL_dw_2, dL_db_2 = gradient(delta_2, a_1)\n",
    "    W2 = W2 - alpha2 * dL_dw_2\n",
    "    b2 = b2 - alpha2 * dL_db_2\n",
    "\n",
    "    dL_dw_1, dL_db_1 = gradient(delta_1, a_0)\n",
    "    W1 = W1 - alpha1 * dL_dw_1\n",
    "    b1 = b1 - alpha1 * dL_db_1\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de optimización Stochastic Gradient Descend consiste en propagar hacia delante nuevamente usando un batch diferente para seguir corrigiendo el modelo. Pero antes, veamos el proceso de una iteración con un batch determinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_ini = np.random.randn(784, 10)\n",
    "b1_ini = np.zeros((10,))\n",
    "W2_ini = np.random.randn(10,10)\n",
    "b2_ini = np.zeros((10,))\n",
    "batch_size = 6\n",
    "x_batch = x_train_reshaped[np.arange(0,batch_size)]\n",
    "y_batch = y_train[np.arange(0,batch_size)]\n",
    "a_0 = x_batch\n",
    "y_batch = encoder(y_batch)#lo codifico en el formato probabilistico\n",
    "z_1,a_1,a_2,delta_2 = Forward_prop(a_0,sigmoid,W1_ini,b1_ini,softmax,W2_ini,b2_ini,y_batch)\n",
    "delta_1 = delta_i(delta_2,sigmoid_p,z_1,W2_ini)\n",
    "W1,b1,W2,b2 = Backward_prop(delta_2,W2_ini,b2_ini,delta_1,W1_ini,b1_ini,a_1,a_0,10,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya sabemos cómo funciona una iteración vamos a crear una funcion optimizador que automatice este proceso barriendo todos los datos de entrenamiento en batches. Pero antes es necesario particionar los datos en batches. La función batch_gen se encargará de ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen (n_batches,x_train_reshaped,y_train):\n",
    "    n_muestras = x_train_reshaped.shape[0]\n",
    "    length_muestrasx = x_train_reshaped.shape[1]\n",
    "    y_train_enc = encoder(y_train)\n",
    "    length_muestrasy = y_train_enc.shape[1]\n",
    "    batch_size = n_muestras/n_batches\n",
    "    batch_size = int(batch_size)\n",
    "\n",
    "    x_batches = np.empty((n_batches,batch_size,length_muestrasx))\n",
    "    y_batches = np.empty((n_batches, batch_size, length_muestrasy))\n",
    "    for i in range(0,n_batches):\n",
    "        x_batches[i,:] = x_train_reshaped[np.arange(i*batch_size,i*batch_size+batch_size).astype(int)]\n",
    "        y_batches[i,:] = y_train_enc[np.arange(i*batch_size,i*batch_size+batch_size).astype(int)]\n",
    "\n",
    "    return x_batches, y_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba de funcionamiento del generador de batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension x_batches:  (10000, 6, 784) . Dimension y_batches: (10000, 6, 10)\n"
     ]
    }
   ],
   "source": [
    "n_batches = 10000\n",
    "x_batches,y_batches = batch_gen(n_batches,x_train_reshaped,y_train)\n",
    "print('Dimension x_batches: ',x_batches.shape,\". Dimension y_batches:\", y_batches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También quiero calcular las perdidas L en la ultima capa para pasarlas por la línea de comandos y revisar si el modelo está aprendiendo correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(a_2,y_batch):\n",
    "    return np.mean(np.sum(-y_batch*np.log(a_2),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importo una librería para mostrar el proceso de entrenamiento en el epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo la función de entrenamiento que se encargará de entrenar la red neuronal recorriendo todos los datos de entrenamiento en batches y actualizando los pesos de las neuronas. \n",
    "\n",
    "Optimizer particiona los datos en batches, calcula las salidas de cada capa, los errores de cada capa y actualiza los pesos de las neuronas, es decir hace el forward y el backward propagation para todos los datos segmentados en batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(W1_ini,b1_ini,W2_ini,b2_ini,x_train_reshaped,y_train,n_batches,alpha1,alpha2):\n",
    "    x_batches,y_batches = batch_gen(n_batches,x_train_reshaped,y_train)\n",
    "    W1, b1 = W1_ini, b1_ini\n",
    "    W2, b2 = W2_ini, b2_ini\n",
    "    for n_batch in tqdm(range(n_batches), desc=\"Training Progress\", unit=\"batch\"):\n",
    "        y_batch = y_batches[n_batch]\n",
    "        a_0 = x_batches[n_batch]\n",
    "        z_1,a_1,a_2,delta_2 = Forward_prop(a_0,sigmoid,W1,b1,softmax,W2,b2,y_batch)\n",
    "        delta_1 = delta_i(delta_2,sigmoid_p,z_1,W2)\n",
    "        W1,b1,W2,b2 = Backward_prop(delta_2,W2,b2,delta_1,W1,b1,a_1,a_0,alpha1,alpha2)\n",
    "    L = loss(a_2,y_batch)\n",
    "    return W1,b1,W2,b2,L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a llamar a la funcion a que haga la optimizacion epoch-veces.\n",
    "\n",
    "Los hiperparámetros son: \n",
    "- learning rate = 1\n",
    "- epochs = 15\n",
    "- n_batches = 1000\n",
    "\n",
    "Se pueden modificar para mejorar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1000/1000 [00:03<00:00, 276.87batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 , Loss =  0.6670155613569785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1000/1000 [00:03<00:00, 267.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 , Loss =  0.43307003979319636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1000/1000 [00:03<00:00, 280.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 , Loss =  0.27363857421299664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1000/1000 [00:03<00:00, 284.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 , Loss =  0.23372585425706568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1000/1000 [00:03<00:00, 298.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 , Loss =  0.20308651095831234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "W1_ini = np.random.randn(784, 10)\n",
    "b1_ini = np.zeros((10,))\n",
    "W2_ini = np.random.randn(10,10)\n",
    "b2_ini = np.zeros((10,))\n",
    "epoch = 10\n",
    "for i in range(epoch):\n",
    "    W1,b1,W2,b2,L = optimizer(W1_ini,b1_ini,W2_ini,b2_ini,x_train_reshaped,y_train,n_batches=1000,alpha1=0.5,alpha2=0.5)\n",
    "    W1_ini,b1_ini,W2_ini,b2_ini = W1,b1,W2,b2\n",
    "    print('epoch',i+1,', Loss = ',L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: debe observarse que la función de pérdida Loss disminuye con el tiempo, lo que indica que el modelo está aprendiendo. Un buen valor de pérdida en este caso es 0.15.\n",
    "\n",
    "\n",
    "Guardo las matrices de pesos y bias en un archivo para no tener que volver a entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('W1.npy', W1)\n",
    "np.save('b1.npy', b1)\n",
    "np.save('W2.npy', W2)\n",
    "np.save('b2.npy', b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargo los pesos y bias guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=np.load('W1.npy')\n",
    "b1=np.load('b1.npy')\n",
    "W2=np.load('W2.npy')\n",
    "b2=np.load('b2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino una función que me permita predecir un número a partir de una imágen (básicamente forward propagation con los pesos optimizados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1,b1,W2,b2,x_test_reshaped):\n",
    "    a_0 = x_test_reshaped\n",
    "    z_1 = np.matmul(a_0,W1) + b1\n",
    "    a_1 = sigmoid(z_1)\n",
    "    z_2 = np.matmul(a_1,W2) + b2\n",
    "    a_2 = softmax(z_2)\n",
    "    return a_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo un decodificador para las predicciones, para faciliar la comprobación de los resultados con y_test.\n",
    "\n",
    "Decode convierte el vector de probabilidades en un número entero del 0 al 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(y):\n",
    "    return np.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez el modelo ha sido entrenado, se realizan las predicciones y se estima la tasa de acierto.\n",
    "\n",
    "$$tasa = 1 -\\frac{errores}{predicciones}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de acierto = 0.888\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(W1,b1,W2,b2,x_test_reshaped)\n",
    "n_err = 0\n",
    "m = y_pred.shape[0]\n",
    "for i in range(m):\n",
    "    if decode(y_pred)[i] != y_test[i]:\n",
    "        n_err += 1\n",
    "tasa_acierto = 1 - n_err/m\n",
    "print('Tasa de acierto =',tasa_acierto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
